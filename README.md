# intro to llma  

这项目仅为freecodecamp 课程代码,[讲解transformer架构简单实现](https://www.youtube.com/watch?v=UU1WVnMk4E8),但是还是很不错的入门教程  

文件bigranma.ipynb利用pytorch实现了简单的字符生成器,而文件gpt则在bigranma的基础上实现了一个简单的transformer架构.包括Decoder,多头注意力(MultiHeadAttention),前馈网络(Feed Forward)等等  

### TODOS
1.保存模型,  
...
### 参考资料:
1.[Transformer 模型详解](https://zhuanlan.zhihu.com/p/338817680)
2.[Attention is All You Need](https://arxiv.org/abs/1706.03762v7)
3.[Transformer 李宏毅](https://www.youtube.com/watch?v=ugWDIIOHtPA&list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&index=61)